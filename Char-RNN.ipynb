{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Lab6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5PInJIe1qPBn"},"source":["<a \n","href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n","  target=\"_parent\">\n","  <img\n","    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n","    alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"cksgAH12XRjV"},"source":["# Lab 6: Sequence-to-sequence models\n","\n","### Description:\n","For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n","\n","This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n","\n","### Deliverable:\n","- Fill in the code for the RNN (using PyTorch's built-in GRU).\n","- Fill in the training loop\n","- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n","- Implement your own GRU cell.\n","- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n","\n","### Grading Standards:\n","- 20% Implementation the RNN\n","- 20% Implementation training loop\n","- 20% Implementation of evaluation loop\n","- 20% Implementation of your own GRU cell\n","- 20% Training of your RNN on a domain of your choice\n","\n","### Tips:\n","- Read through all the helper functions, run them, and make sure you understand what they are doing\n","- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n","- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n","\n","### Example Output:\n","An example of my final samples are shown below (more detail in the\n","final section of this writeup), after 150 passes through the data.\n","Please generate about 15 samples for each dataset.\n","\n","<code>\n","And ifte thin forgision forward thene over up to a fear not your\n","And freitions, which is great God. Behold these are the loss sub\n","And ache with the Lord hath bloes, which was done to the holy Gr\n","And appeicis arm vinimonahites strong in name, to doth piseling \n","And miniquithers these words, he commanded order not; neither sa\n","And min for many would happine even to the earth, to said unto m\n","And mie first be traditions? Behold, you, because it was a sound\n","And from tike ended the Lamanites had administered, and I say bi\n","</code>\n"]},{"cell_type":"markdown","metadata":{"id":"c2i_QpSsWG4c"},"source":["---\n","\n","## Part 0: Readings, data loading, and high level training\n","\n","---\n","\n","There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n","\n","* Read the following\n","\n","> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (You will be implementing the decoder, not the encoder, as we are not doing sequence-to-sequence translation.)\n","* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n","\n","\n","NOTES:\n","\n","[This](https://www.youtube.com/watch?v=kXQu2Q__3Tw) ta review was super helpful in understanding the lab.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7bdZWxvJrsx","executionInfo":{"status":"ok","timestamp":1634315690009,"user_tz":360,"elapsed":4112,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"945c1ce1-106a-409e-ccc1-beb76aeb40a9"},"source":["! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n","! tar -xzf text_files.tar.gz\n","! pip install unidecode\n","! pip install torch\n","\n","import unidecode\n","import string\n","import random\n","import re\n"," \n","import pdb\n"," \n","all_characters = string.printable\n","n_characters = len(all_characters)\n","file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n","file_len = len(file)\n","print('file_len =', file_len)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-15 10:34:45--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n","Resolving piazza.com (piazza.com)... 3.221.194.245, 52.44.149.188, 52.54.75.23, ...\n","Connecting to piazza.com (piazza.com)|3.221.194.245|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n","--2021-10-15 10:34:46--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n","Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.249.200.14, 13.249.200.70, 13.249.200.16, ...\n","Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.249.200.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1533290 (1.5M) [application/x-gzip]\n","Saving to: ‘./text_files.tar.gz’\n","\n","./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.1s    \n","\n","2021-10-15 10:34:46 (10.3 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n","\n","Collecting unidecode\n","  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 3.9 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.2\n","Requirement already satisfied: torch in ./.local/lib/python3.8/site-packages (1.9.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.10.0.0)\n","file_len = 2579888\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxBeKeNjJ0NQ","executionInfo":{"status":"ok","timestamp":1634315690052,"user_tz":360,"elapsed":42,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"0c7c28e5-df6b-435c-9fe4-8242f760d4d4"},"source":["chunk_len = 200\n"," \n","def random_chunk():\n","  start_index = random.randint(0, file_len - chunk_len)\n","  end_index = start_index + chunk_len + 1\n","  return file[start_index:end_index]\n","  \n","print(random_chunk())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["still stung by the scorn and suspicion in that cold voice. \n","\n","'Little service, no doubt, will so great a lord of Men think to find in a \n","hobbit, a halfling from the northern Shire; yet such as it is, I \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On0_WitWJ99e","executionInfo":{"status":"ok","timestamp":1634315690435,"user_tz":360,"elapsed":382,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"ca73db8a-4bf5-44de-fe69-cf9062622a3c"},"source":["import torch\n","# Turn string into list of longs\n","def char_tensor(string):\n","  tensor = torch.zeros(len(string)).long()\n","  for c in range(len(string)):\n","      tensor[c] = all_characters.index(string[c])\n","  return tensor\n","\n","print(char_tensor('abcDEF'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10, 11, 12, 39, 40, 41])\n"]}]},{"cell_type":"markdown","metadata":{"id":"CYJPTLcaYmfI"},"source":["---\n","\n","## Part 4: Creating your own GRU cell \n","\n","**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n","\n","---\n","\n","The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n","\n","Please do not look at the documentation's code for the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n","\n","**TODO:**\n","* Create a custom GRU cell\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","metadata":{"id":"aavAv50ZKQ-F"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#this module is one of the little GRU squares on the diagram.\n","class GRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(GRUCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.sig = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","\n","        self.wr = nn.Linear(input_size+hidden_size, hidden_size)\n","        self.wz = nn.Linear(input_size+hidden_size, hidden_size)\n","        self.wn = nn.Linear(input_size+hidden_size, hidden_size)\n","\n","    def forward(self,x_input, prev_hidden):\n","        # Each layer does the following:\n","        #NOTE: Wx + b is the form of a linear layer.\n","        # r_t = sigmoid(concat(W_ir*x_t + b_ir, W_hr*h_(t-1) + b_hr))\n","        # stack everything into one operation:\n","        # r_t ===> concat(W_ir, W_hr) @ concat(x_t, h_(t-1)) + concat(b_ir, b_hr)\n","        # (1,1,input_size) concat with (1,1,hidden_size)\n","        r_t = self.sig(self.wr(torch.cat((x_input, prev_hidden), dim=2)))\n","\n","        # z_t = sigmoid(concat(W_iz*x_t + b_iz, W_hz*h_(t-1) + b_hz))\n","        z_t = self.sig(self.wz(torch.cat((x_input, prev_hidden), dim=2)))\n","\n","        # n_t = tanh(concat(W_in*x_t + b_in, r_t**(W_hn*h_(t-1) + b_hn)))\n","        n_t = self.tanh(self.wn(torch.cat((torch.mul(r_t, prev_hidden), x_input), dim=2)))\n","\n","        # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n","        h_t = torch.mul(1-z_t, n_t) + torch.mul(z_t, prev_hidden)\n","        # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n","\n","        #return two things because they are each going to different places.\n","        return h_t, h_t\n","\n","class GRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRU, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # cannot use an array here. Must use torch's modulelist to register all of the parameters in each layer with the optimizer for backprop\n","        self.gru_cells = nn.ModuleList([])\n","        for layer in range(self.num_layers):\n","            gru_input_size = self.input_size if layer == 0 else hidden_size\n","            self.gru_cells.append(GRUCell(gru_input_size, hidden_size))\n","  \n","    def forward(self, inputs, hidden):\n","        #first GRU cell pass through\n","        outputs, hiddens = self.gru_cells[0](inputs, hidden[0:1])\n","        #for the rest of the layers\n","        for layer in range(1, self.num_layers):\n","            outputs, hidden_layer = self.gru_cells[layer](outputs, hidden[layer:layer+1])\n","            #concat with hiddens to keep a record to return at the end\n","            hiddens = torch.cat((hiddens, hidden_layer), dim=0)\n","    \n","        return outputs, hiddens #(1,1,200), (3,1,200)\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtXdX-B_WiAY"},"source":["---\n","\n","##  Part 1: Building a sequence to sequence model\n","\n","---\n","\n","Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n","\n","We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n","\n","\n","**TODO:**\n","* Create an RNN class that extends from nn.Module.\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","metadata":{"id":"d6tNdEnzWj5F"},"source":["\n","import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","        super(RNN, self).__init__()\n","        self.input_size = input_size #vocabulary size\n","        self.hidden_size = hidden_size #embedding dimension\n","        self.output_size = output_size #\n","        self.n_layers = n_layers\n","        \n","        # more stuff here...\n","        #pytorch embedding is a way to encode a word as vector and has shape (vocab size x embedding dim)\n","        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n","\n","        self.relu = nn.ReLU()\n","\n","        #GRU expects (batch x input size x hidden size) and returns (batch x input_size x hidden_size)\n","        self.gru = GRU(input_size=self.hidden_size,\n","                        hidden_size=self.hidden_size,\n","                        num_layers=self.n_layers)\n","\n","        self.to_output_shape = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input_char, hidden):\n","        # by reviewing the documentation, construct a forward function that properly uses the output\n","        # of the GRU\n","        embedded = self.embedding(input_char).view(1,1,-1)\n","        output, hidden = self.gru(embedded, hidden)\n","        out_decoded = self.relu(self.to_output_shape(output))\n","        return out_decoded, hidden\n","\n","    def init_hidden(self):\n","        return torch.zeros(self.n_layers, 1, self.hidden_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrhXghEPKD-5"},"source":["def random_training_set():    \n","  chunk = random_chunk()\n","  inp = char_tensor(chunk[:-1])\n","  target = char_tensor(chunk[1:])\n","  return inp, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpiGObbBX0Mr"},"source":["---\n","\n","## Part 2: Sample text and Training information\n","\n","---\n","\n","We now want to be able to train our network, and sample text after training.\n","\n","This function outlines how training a sequence style network goes. \n","\n","**TODO:**\n","* Fill in the pieces.\n","\n","**DONE:**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2ALC3Pf8Kbsi"},"source":["# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n","def train(input_tensor, target_tensor):\n","    # initialize hidden layers, set up gradient and loss\n","    decoder_optimizer.zero_grad()\n","    hidden = decoder.init_hidden()\n","    loss = 0\n","\n","    # print(\"\\nINPUT:{}\".format(input_tensor))\n","    # print(\"TARGET:{}\".format(target_tensor))\n","\n","    for c in range(chunk_len):\n","        # print(\"\\nCHAR_IN: {}\".format(input_tensor[c]))\n","        # print(\"CHAR_TARGET: {}\".format(target_tensor[c]))\n","        #make a prediction given char_in\n","        #PROBLEM WAS HERE. I wasn't overwriting hidden in each loop, so I was just using the initial hidden each time. \n","        y_hat, hidden = decoder(input_tensor[c], hidden)\n","        #reshapce so CEL is happy\n","        #add up loss between predicted distribution and actual one\n","        loss += criterion(y_hat.squeeze(0), target_tensor[c].unsqueeze(0))\n","\n","    #optimize\n","    loss.backward()\n","    decoder_optimizer.step()\n","\n","    #return average score\n","    loss_score = loss.item()/len(input_tensor)\n","    return loss_score\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EN06NUu3YRlz"},"source":["---\n","\n","## Part 3: Sample text and Training information\n","\n","---\n","\n","You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n","\n","If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n","\n","**TODO:**\n","* Fill out the evaluate function to generate text frome a primed string\n","\n","**DONE:**\n","\n"]},{"cell_type":"code","metadata":{"id":"B-bp-OZ1KjNh"},"source":["def sample_outputs(output, temperature):\n","    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n","    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n","    # As temperature approaches infinity, this sampling function becomes a purely random choice\n","    #NOTE: choose highest probability with the highest probability, but not always.\n","    # sometimes \"the\" or sometimes \"that\"\n","    return torch.multinomial(torch.exp(output / temperature), 1)\n","\n","def evaluate(prime_str='A', predict_len=200, temperature=0.8):\n","    with torch.no_grad():\n","        # initialize hidden variable, initialize other useful variables\n","        hidden = decoder.init_hidden()\n","        prime_chars = char_tensor(prime_str)\n","\n","        #go through the whole primer string \n","        # (minus 1 b/c the last char of primer is first input to prediction)\n","        for c in range(len(prime_chars)-1):\n","            #pass hiddens through\n","            _, hidden = decoder(prime_chars[c], hidden)\n","\n","        #input to prediction part of string is the last char of the primer string\n","        eval_char = prime_chars[-1]\n","\n","        for p in range(predict_len):\n","            next_char_prob, hidden = decoder(eval_char, hidden) #next_char_prob is 1x1x100\n","            sampled_index = sample_outputs(next_char_prob.view(-1), temperature=temperature) #just pass in last dim\n","            #get actual char from its index\n","            char_choice = all_characters[sampled_index]\n","            #update to be the next char in the sequence for the next loop\n","            eval_char = char_tensor(char_choice)\n","            #add predicted char to end of prediction string\n","            prime_str += char_choice\n","\n","    return prime_str"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Du4AGA8PcFEW"},"source":["---\n","\n","## Part 4: (Create a GRU cell, requirements above)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"GFS2bpHSZEU6"},"source":["\n","---\n","\n","## Part 5: Run it and generate some text!\n","\n","---\n","\n","\n","**TODO:** \n","* Create some cool output\n","\n","**DONE:**\n","\n","\n","\n","\n","Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n","\n","---\n","\n"," G:\n"," \n"," Gandalf was decrond. \n","'All have lord you. Forward the road at least walk this is stuff, and \n","went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n","row hrough. In  \n","\n"," lo:\n"," \n"," lost death it. \n","'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n","'Yes, as you there were remembaused to seen their pass, when? What \n","said here, such seven an the sear \n","\n"," lo:\n"," \n"," low, and frod to keepn \n","Came of their most. But here priced doubtless to an Sam up is \n","masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n","the like \n","\n"," I:\n"," \n"," I had been the \n","in his eyes with the perushed to lest, if then only the ring and the legended \n","of the less of the long they which as the \n","enders of Orcovered and smood, and the p \n","\n"," I:\n"," \n"," I they were not the lord of the hoomes. \n","Home already well from the Elves. And he sat strength, and we \n","housed out of the good of the days to the mountains from his perith. \n","\n","'Yess! Where though as if  \n","\n"," Th:\n"," \n"," There yarden \n","you would guard the hoor might. Far and then may was \n","croties, too began to see the drumbred many line \n","and was then hoard walk and they heart, and the chair of the \n","Ents of way, might was \n","\n"," G:\n"," \n"," Gandalf \n","been lat of less the round of the stump; both and seemed to the trees and perished they \n","lay are speered the less; and the wind the steep and have to she \n","precious. There was in the oonly went \n","\n"," wh:\n"," \n"," which went out of the door. \n","Hull the King and of the The days of his brodo \n","stumbler of the windard was a thing there, then it been shining langing \n","to him poor land. They hands; though they seemed ou \n","\n"," ra:\n"," \n"," rather,' have all the least deather \n","down of the truven beginning to the house of sunk. \n","'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n","'I wander trust horn, and there were not, it  \n","\n"," I:\n"," \n"," I can have no mind \n","together! Where don't may had one may little blung \n","terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n","not stopped great the out them forms. On they she lo \n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"-nXFeCmdKodw"},"source":["import time\n","n_epochs = 5000\n","print_every = 500\n","plot_every = 10\n","hidden_size = 100\n","n_layers = 3\n","lr = 0.0007\n"," \n","decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n"," \n","start = time.time()\n","all_losses = []\n","loss_avg = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKfozqw-6eqb","executionInfo":{"status":"ok","timestamp":1634324518827,"user_tz":360,"elapsed":721786,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"b71b42d0-97ba-4626-d97a-a2d891041a9b"},"source":["from tqdm import tqdm\n","for epoch in tqdm(range(1, n_epochs + 1)):\n","    loss_ = train(*random_training_set())    \n","    loss_avg += loss_\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","        print(evaluate('Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|████                                    | 501/5000 [01:12<10:59,  6.83it/s]"]},{"output_type":"stream","name":"stdout","text":["[72.52430748939514 (500 10%) 2.7162]\n","Why as, Uoll de18y day are, ?#erNrer ~illmald efer, si$he \n","moEPer \n","a2eriz) sull sous |ordiwhi\fgursy \n","r \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|███████▊                               | 1001/5000 [02:23<10:01,  6.65it/s]"]},{"output_type":"stream","name":"stdout","text":["[143.84948182106018 (1000 20%) 2.5733]\n","Whert was \n","\n","Furdeld. \n","\n","\n","\n","\n","\n","\n","\n","fet dart. At the goloud the 9oud the \n","ap, \n","of as dumry is \n","\n","vou_thy walga \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███████████▋                           | 1501/5000 [03:35<09:06,  6.40it/s]"]},{"output_type":"stream","name":"stdout","text":["[216.25750255584717 (1500 30%) 2.1070]\n","Whe, at \n","his }old adowto feress. Affer all the mase seemar *idow. \n","\n","\n","me sat the more the said, ~wo the \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|███████████████▌                       | 2001/5000 [04:48<07:26,  6.72it/s]"]},{"output_type":"stream","name":"stdout","text":["[288.57297229766846 (2000 40%) 1.9757]\n","Whing on them. \n","\n","\n","\n","\n","Whould in they han. |our hill aly \n","agold do so ever is moles. qean have \n","hound't s \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|███████████████████▌                   | 2501/5000 [06:01<06:04,  6.85it/s]"]},{"output_type":"stream","name":"stdout","text":["[361.609512090683 (2500 50%) 1.6634]\n","Whil then't,' wele =eary white and alroud. \n","\n","deast of they wold new great he nesy so wellodid and they \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|███████████████████████▍               | 3001/5000 [07:13<04:58,  6.70it/s]"]},{"output_type":"stream","name":"stdout","text":["[434.41780614852905 (3000 60%) 1.7143]\n","Whaters now for the grepery, and the some of then he gath. \n","\n","I was shore a while be if to for to seep, \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████████████████████████▎           | 3501/5000 [08:26<03:38,  6.87it/s]"]},{"output_type":"stream","name":"stdout","text":["[506.6041090488434 (3500 70%) 2.0296]\n","Where it have told sime one you \n","\n","Right \n","their tade and remain, and all had now 8rosen mind an like al \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|███████████████████████████████▏       | 4001/5000 [09:35<02:28,  6.75it/s]"]},{"output_type":"stream","name":"stdout","text":["[576.2993803024292 (4000 80%) 1.5003]\n","What forgething lands, and we glopes alrost and line most away \n","the deep to still. Frodo whinly old mo \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|███████████████████████████████████    | 4501/5000 [10:49<01:19,  6.32it/s]"]},{"output_type":"stream","name":"stdout","text":["[649.5312356948853 (4500 90%) 1.6279]\n","Where that I what be some the was stood \n","for and path some forther, and they pattle it, but he has bee \n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 5000/5000 [12:01<00:00,  6.93it/s]"]},{"output_type":"stream","name":"stdout","text":["[722.2638111114502 (5000 100%) 1.7903]\n","Whiling \n","they said its and breaving all the shadows of fives of the \n","his there. He to down \n","white stri \n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"RYklKj_1arGX","executionInfo":{"status":"ok","timestamp":1634324525879,"user_tz":360,"elapsed":58,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"fdb15c59-a7e3-42b7-b5bd-3cb5d598ac10"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(all_losses)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7faf631beb50>]"]},"metadata":{},"execution_count":118},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ee0so6aKJ5L8","executionInfo":{"status":"ok","timestamp":1634324530319,"user_tz":360,"elapsed":550,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"1f2ebe7a-570e-46d7-d1c9-2ebbe271608d"},"source":["for i in range(10):\n","  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","  start = random.randint(0,len(start_strings)-1)\n","  print(start_strings[start])\n","#   all_characters.index(string[c])\n","  print(evaluate(start_strings[start], 200), '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" ca\n"," can younking \n","ongue that ;on the darged hear the green been the the right that the how \n","that then he heard not litten on sleater. More do not was so there is nom in a fout in the great great shall sudde \n","\n"," G\n"," Gandalf. If might then he was \n","and through a fagen \n","both. Srow son the left to \n","dying you was leed heart. \n","\n","'I long it \n","not \n","not is \n","rain than a wift up as he hand on his aras, and seen him, and so am  \n","\n"," ca\n"," came \n","treak hess. \n","\n","''And so hoYe hope the straBped aorobs. >em no lough \n","the soft hyward of the Kened to the near. 'It was now yell not straught all ever one they \n","with he longer. 'I tied \n","near. 'I wor \n","\n"," I \n"," I may. 'Now i roof a brand step string, \n","and not whtere him to the world the tenty of I many had said out \n","some is }lands wordly the weathing of the 'tale and heard, and soog a stringing with \n","they had  \n","\n"," lo\n"," long of a streamed the \n","put. He last for the hose a \n","passed into a shear, bag on with the last, and has I stirst and first that down the \n","dark stone. \n","\n","''-Sam the step \n","stand, and he said, I hole to so  \n","\n"," wh\n"," white the tend. 'You may did not heeping at the home, drouse see these { {stant about little sent of the losted your they have shose the enstep what he %ust were neared the \n","like as the \n","promid a loud u \n","\n"," ra\n"," ran this been the stretter of the \n","his far fanging untain. jady but who shations to him \n","behind as&ern him in the floor and were their draiss to the voips with the part \n","time to the meeps in the Brike a \n","\n"," I \n"," I right have strange \n","hen have say. 'As have then he sat %y, those that I lowed of the \n","some things \n","the |oose. \n","\n","'But \n","you up, and in from the fra, as them alight will they strange that the \n","light like \n","\n"," ca\n"," came up the 8ries as th; we mush eyes of the \n","a prittle then it from up answertured the bashed out and him strider. His for us a lother his puts. His were like he heard it, I down not starry but they ha \n","\n"," ca\n"," came and and he pass. \n","\n","Frodo had any bent they had we, \n","\n","\n","\n","\n","^low happ of \n","{hat how white a shint thinks with all the his \n","hill, and \n","there was sone of his frees. If the ride should the \n","with a great ha \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"YJhgDc2IauPE"},"source":["---\n","\n","## Part 6: Generate output on a different dataset\n","\n","---\n","\n","**TODO:**\n","\n","* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n","\n","* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n","\n","**DONE:**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8HwsvgjFIycN"},"source":["I downloaded state of the union addresses of all presidents. \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WOlhXfCWIspy","executionInfo":{"status":"ok","timestamp":1634326838982,"user_tz":360,"elapsed":1101,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"8b1f92b4-26c1-4489-f344-c202695e72d4"},"source":["!pip install opendatasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opendatasets in ./.local/lib/python3.8/site-packages (0.1.20)\r\n","Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from opendatasets) (4.62.1)\r\n","Requirement already satisfied: kaggle in ./.local/lib/python3.8/site-packages (from opendatasets) (1.5.12)\r\n","Requirement already satisfied: click in ./.local/lib/python3.8/site-packages (from opendatasets) (8.0.1)\r\n","Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2.22.0)\r\n","Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (1.14.0)\r\n","Requirement already satisfied: python-slugify in ./.local/lib/python3.8/site-packages (from kaggle->opendatasets) (5.0.2)\r\n","Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2019.11.28)\r\n","Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2.7.3)\r\n","Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (1.25.8)\r\n","Requirement already satisfied: text-unidecode>=1.3 in ./.local/lib/python3.8/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\r\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkEuORP1Ls4l","executionInfo":{"status":"ok","timestamp":1634326862708,"user_tz":360,"elapsed":5,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"7e4efa5a-6db9-47c2-933a-f712e0644729"},"source":["import opendatasets as od\n","od.download(\"https://www.kaggle.com/jyronw/us-state-of-the-union-addresses-1790-2019\")\n","#{\"username\":\"curtiscjohnson\",\"key\":\"a1464c2d3670441fa3739c4d40484c57\"}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping, found downloaded files in \"./us-state-of-the-union-addresses-1790-2019\" (use force=True to force download)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"rujKS98aMM9J","executionInfo":{"status":"ok","timestamp":1634326862945,"user_tz":360,"elapsed":69,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"d3efd23e-ca29-418f-8f0c-b22978756c9c"},"source":["import pandas as pd\n","\n","data = pd.read_csv(\"./us-state-of-the-union-addresses-1790-2019/state_ofthe_union_texts.csv\")\n","\n","data\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             President  ...                                               Text\n","0    George Washington  ...  ['I embrace with great satisfaction the opport...\n","1    George Washington  ...  ['Fellow-Citizens of the Senate and the House ...\n","2    George Washington  ...  ['Fellow-Citizens of the Senate and the House ...\n","3    George Washington  ...  ['Fellow-Citizens of the Senate and of the Hou...\n","4    George Washington  ...  ['Fellow Citizens of the Senate and of the Hou...\n","..                 ...  ...                                                ...\n","214       Barack Obama  ...  ['Mr. Speaker, Mr. Vice President, Members of ...\n","215       Barack Obama  ...  ['Mr. Speaker, Mr. Vice President, Members of ...\n","216       Donald Trump  ...  ['Mr. Speaker, Mr. Vice President, Members of ...\n","217       Donald Trump  ...  ['Mr. Speaker, Mr. Vice President, Members of ...\n","218       Donald Trump  ...  ['Madam Speaker, Mr. Vice President, Members o...\n","\n","[219 rows x 4 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>President</th>\n","      <th>Year</th>\n","      <th>Title</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>George Washington</td>\n","      <td>1790</td>\n","      <td>First State of the Union Address</td>\n","      <td>['I embrace with great satisfaction the opport...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>George Washington</td>\n","      <td>1790</td>\n","      <td>Second State of the Union Address</td>\n","      <td>['Fellow-Citizens of the Senate and the House ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>George Washington</td>\n","      <td>1791</td>\n","      <td>Third State of the Union Address</td>\n","      <td>['Fellow-Citizens of the Senate and the House ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>George Washington</td>\n","      <td>1792</td>\n","      <td>Fourth State of the Union Address</td>\n","      <td>['Fellow-Citizens of the Senate and of the Hou...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>George Washington</td>\n","      <td>1793</td>\n","      <td>Fifth State of the Union Address</td>\n","      <td>['Fellow Citizens of the Senate and of the Hou...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>214</th>\n","      <td>Barack Obama</td>\n","      <td>2015</td>\n","      <td>Barack Obama's Seventh State of the Union Address</td>\n","      <td>['Mr. Speaker, Mr. Vice President, Members of ...</td>\n","    </tr>\n","    <tr>\n","      <th>215</th>\n","      <td>Barack Obama</td>\n","      <td>2016</td>\n","      <td>Barack Obama's Eighth State of the Union Address</td>\n","      <td>['Mr. Speaker, Mr. Vice President, Members of ...</td>\n","    </tr>\n","    <tr>\n","      <th>216</th>\n","      <td>Donald Trump</td>\n","      <td>2017</td>\n","      <td>Donald Trump's First State of the Union Address</td>\n","      <td>['Mr. Speaker, Mr. Vice President, Members of ...</td>\n","    </tr>\n","    <tr>\n","      <th>217</th>\n","      <td>Donald Trump</td>\n","      <td>2018</td>\n","      <td>Donald Trump's Second State of the Union Address</td>\n","      <td>['Mr. Speaker, Mr. Vice President, Members of ...</td>\n","    </tr>\n","    <tr>\n","      <th>218</th>\n","      <td>Donald Trump</td>\n","      <td>2019</td>\n","      <td>Donald Trump's Third State of the Union Address</td>\n","      <td>['Madam Speaker, Mr. Vice President, Members o...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>219 rows × 4 columns</p>\n","</div>"]},"metadata":{},"execution_count":165}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wa2Tbs9qMy9l","executionInfo":{"status":"ok","timestamp":1634326864560,"user_tz":360,"elapsed":6,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"7db777d8-9117-4625-8102-f4a0bb687a98"},"source":["type(data[\"Text\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pandas.core.series.Series"]},"metadata":{},"execution_count":166}]},{"cell_type":"code","metadata":{"id":"ldtGqqL9at3T"},"source":["text = \"\"\n","for index, string in data['Text'].items():\n","    text += string\n","#not gonna worry about special characters like [] or \\ for lines. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-GklrQDcdtfg","executionInfo":{"status":"ok","timestamp":1634327106285,"user_tz":360,"elapsed":8,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"cb1e848a-da10-45d5-f5c0-cee8719f56f4"},"source":["len(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10622271"]},"metadata":{},"execution_count":170}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEA1-5_Xh1UX","executionInfo":{"status":"ok","timestamp":1634327932671,"user_tz":360,"elapsed":1470,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"30440dc8-4af3-4a91-a2e3-3bc1b6a95b45"},"source":["with open(\"SOTU.txt\", 'w') as f:\n","    f.write(text)\n","    f.close()\n","\n","file = unidecode.unidecode(open('SOTU.txt').read())\n","file_len = len(file)\n","print('file_len =', file_len)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["file_len = 10624563\n"]}]},{"cell_type":"code","metadata":{"id":"h7m95H3Ufr1_"},"source":["n_epochs = 5000\n","print_every = 500\n","plot_every = 10\n","hidden_size = 100\n","n_layers = 3\n","lr = 0.0007\n"," \n","decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n"," \n","start = time.time()\n","all_losses = []\n","loss_avg = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mdU0is-f1P3","executionInfo":{"status":"ok","timestamp":1634328656325,"user_tz":360,"elapsed":654449,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"fb95fa93-0f88-4bd4-d653-b94db806c5d3"},"source":["for epoch in tqdm(range(1, n_epochs + 1)):\n","    loss_ = train(*random_training_set())    \n","    loss_avg += loss_\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","        print(evaluate('Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|████                                    | 501/5000 [01:06<10:20,  7.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[83.40230989456177 (500 10%) 2.9223]\n","Whp hn opodoiNnelJ eP, on ceoSnif to ich Gosotabn ononoseefian nhr sinisas] are o d cs oat noof t seit \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|███████▊                               | 1001/5000 [02:12<08:50,  7.54it/s]"]},{"output_type":"stream","name":"stdout","text":["[149.8746247291565 (1000 20%) 2.1523]\n","Whe surdin me the neite of Reuld <ist at prreer of the nand proats and of the alt date epead of the I  \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███████████▋                           | 1501/5000 [03:17<07:44,  7.54it/s]"]},{"output_type":"stream","name":"stdout","text":["[214.3344020843506 (1500 30%) 1.8826]\n","Wher sact must and ale eneteet of the clas a part all connersional creats of the port the re~uers copc \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|███████████████▌                       | 2001/5000 [04:20<06:47,  7.36it/s]"]},{"output_type":"stream","name":"stdout","text":["[278.12730169296265 (2000 40%) 1.9512]\n","WhinF the present of that protess to procities the pudded prolt fhance dispensed to be prifitaratus th \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|███████████████████▌                   | 2501/5000 [05:25<05:34,  7.48it/s]"]},{"output_type":"stream","name":"stdout","text":["[343.12504863739014 (2500 50%) 1.8237]\n","Wher intustiment of the +rantural so to not of the mor of the shortene is that the convent made in a f \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|███████████████████████▍               | 3001/5000 [06:31<04:41,  7.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[408.56090474128723 (3000 60%) 1.6576]\n","When act a mameany are chibiliminause, to keriary interest shall depart in should ,eceA1t year a po\u000bt  \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████████████████████████▎           | 3501/5000 [07:35<03:23,  7.38it/s]"]},{"output_type":"stream","name":"stdout","text":["[472.77331042289734 (3500 70%) 1.6831]\n","Wher, and the accomped party comits poshefties of the amilation of must to it recentries \rekuld of all \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|███████████████████████████████▏       | 4001/5000 [08:41<02:16,  7.34it/s]"]},{"output_type":"stream","name":"stdout","text":["[538.6062543392181 (4000 80%) 1.8230]\n","Whoutal act to presult of the last is in those(, and from the can and the ;econded to be fraitment of  \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|███████████████████████████████████    | 4501/5000 [09:47<01:10,  7.10it/s]"]},{"output_type":"stream","name":"stdout","text":["[604.3614726066589 (4500 90%) 1.6088]\n","Which it thereors to Deplected, and ,00, ;uch such ficitical reasult from the returnity of leat the Yu \n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 5000/5000 [10:54<00:00,  7.64it/s]"]},{"output_type":"stream","name":"stdout","text":["[671.7685198783875 (5000 100%) 1.4674]\n","Whice the sources some and ofly men pland to prive the proposed is stilling the breaty at their expres \n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvH8bCJ8f9DY","executionInfo":{"status":"ok","timestamp":1634339926799,"user_tz":360,"elapsed":563,"user":{"displayName":"Curtis Johnson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsB0wHbXHMLyZz_K365v78kxDD83dh_IvWfD6EZgE=s64","userId":"08792276826441947747"}},"outputId":"46ec892d-d6e1-4c00-b0fd-eb233a7c6a07"},"source":["for i in range(10):\n","  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","  start = random.randint(0,len(start_strings)-1)\n","  print(start_strings[start])\n","#   all_characters.index(string[c])\n","  print(evaluate(start_strings[start], 200), '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" wh\n"," which bether but its former of the interminity interest and for they cher not herise the loOge the caused and value of Xxagy the economy of .ure resist of the fund by our \felication is refendments by at \n","\n"," ra\n"," ray and the praged to consilies that this labor that the bost has hope breach the a aimiture.', zendor have furnectors are the enforce of the States the 6ther [+fitly and us the stear of its educy of th \n","\n"," he\n"," here and not enount it a vanic considerative for suple.',y Xreed in the our enacted provision and friend met to its bost has relification for thrang the last in#ur and country and commandast in such is  \n","\n"," Th\n"," The bread of pay of the long.', \"at this entired and inend in the righter proves and securing |urred housing, exill this brouse not be are some has Korethen and of our achoopication resourcensiomed that \n","\n"," lo\n"," local pro/urds. Its adriplic \\nfor+ther }here not the property. It interests addecity to affect the country effectment in the him presentional been sates of the purposed that it are inspection of <evote \n","\n"," ra\n"," ray of the not inmlease of a conduction of persons to this it able of those communignted the considerations in the publy for the submitted have bean 9, our the peace depends, have liWe the provides of t \n","\n"," ra\n"," ratures, this from this still our suggeaustisulations and sinst fthat responsices vrimor, than the great against intercation of our dains and it affidy great to the interment should pace of upon the dis \n","\n"," lo\n"," lo\t' centroy and a longers are increase of the has to at discredurent, body of the Sesvation, and of preats on the States for the right other out is in the national states in the purpose the bord to str \n","\n"," he\n"," here to be the and our proper the relations, and the inrade in oning for the finst several as mone dugient of the last in day to messure become and the Interestly or the mune the great be repartments to \n","\n"," I \n"," I necessary bubusing the United States in 9 the ralion progotion and and improver.', \"$Ge/her on the aL its be progress to the dair to incomes from the inflicts of the progress of the commerce made by b \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"ig9VntSxQu8l"},"source":["This is cool. The RNN picked up on important words like \"United States\" and \"economy\". It is also interesting that lots of the longer formal words that are compound words are mixed up -- like \"intercation\", \"effectment\", or \"presentional\". There are still some weird characters floating around and random numbers.\n","\n","The RNN also picked up on punctuation decently. Capital letters generally come after periods, but not after commas or quotations. "]}]}